//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-31294372
// Cuda compilation tools, release 11.7, V11.7.64
// Based on NVVM 7.0.1
//

.version 7.7
.target sm_86
.address_size 64

	// .globl	mul_f64

.visible .entry mul_f64(
	.param .u64 mul_f64_param_0,
	.param .u64 mul_f64_param_1,
	.param .u64 mul_f64_param_2,
	.param .u64 mul_f64_param_3,
	.param .u64 mul_f64_param_4
)
{
	.reg .pred 	%p<25>;
	.reg .b32 	%r<57>;
	.reg .f64 	%fd<7>;
	.reg .b64 	%rd<101>;


	ld.param.u64 	%rd50, [mul_f64_param_0];
	ld.param.u64 	%rd51, [mul_f64_param_1];
	ld.param.u64 	%rd52, [mul_f64_param_2];
	ld.param.u64 	%rd48, [mul_f64_param_3];
	ld.param.u64 	%rd49, [mul_f64_param_4];
	cvta.to.global.u64 	%rd1, %rd52;
	cvta.to.global.u64 	%rd2, %rd51;
	cvta.to.global.u64 	%rd3, %rd50;
	mov.u32 	%r2, %ntid.y;
	mov.u32 	%r3, %ntid.x;
	mul.lo.s32 	%r4, %r3, %r2;
	mov.u32 	%r5, %ntid.z;
	mul.lo.s32 	%r6, %r4, %r5;
	mov.u32 	%r7, %nctaid.x;
	mul.lo.s32 	%r8, %r6, %r7;
	mov.u32 	%r9, %nctaid.y;
	mul.lo.s32 	%r10, %r8, %r9;
	mov.u32 	%r11, %nctaid.z;
	mul.lo.s32 	%r12, %r10, %r11;
	cvt.u64.u32 	%rd4, %r12;
	mov.u32 	%r13, %ctaid.x;
	mov.u32 	%r14, %ctaid.y;
	mad.lo.s32 	%r15, %r13, %r2, %r14;
	mov.u32 	%r16, %ctaid.z;
	mad.lo.s32 	%r17, %r15, %r5, %r16;
	mov.u32 	%r18, %tid.x;
	mad.lo.s32 	%r19, %r17, %r7, %r18;
	mov.u32 	%r20, %tid.y;
	mad.lo.s32 	%r21, %r19, %r9, %r20;
	mov.u32 	%r22, %tid.z;
	mad.lo.s32 	%r23, %r21, %r11, %r22;
	cvt.u64.u32 	%rd5, %r23;
	or.b64  	%rd53, %rd48, %rd49;
	and.b64  	%rd54, %rd53, -4294967296;
	setp.eq.s64 	%p1, %rd54, 0;
	@%p1 bra 	$L__BB0_2;

	rem.u64 	%rd89, %rd48, %rd49;
	bra.uni 	$L__BB0_3;

$L__BB0_2:
	cvt.u32.u64 	%r24, %rd49;
	cvt.u32.u64 	%r25, %rd48;
	rem.u32 	%r26, %r25, %r24;
	cvt.u64.u32 	%rd89, %r26;

$L__BB0_3:
	setp.lt.u64 	%p2, %rd5, %rd48;
	setp.eq.s64 	%p3, %rd89, 0;
	and.pred  	%p4, %p2, %p3;
	@%p4 bra 	$L__BB0_24;
	bra.uni 	$L__BB0_4;

$L__BB0_24:
	setp.gt.u64 	%p17, %rd4, %rd49;
	mov.u64 	%rd96, 1;
	@%p17 bra 	$L__BB0_28;

	and.b64  	%rd76, %rd49, -4294967296;
	setp.eq.s64 	%p18, %rd76, 0;
	@%p18 bra 	$L__BB0_27;

	div.u64 	%rd96, %rd4, %rd49;
	bra.uni 	$L__BB0_28;

$L__BB0_4:
	or.b64  	%rd55, %rd49, %rd48;
	and.b64  	%rd56, %rd55, -4294967296;
	setp.eq.s64 	%p5, %rd56, 0;
	@%p5 bra 	$L__BB0_6;

	rem.u64 	%rd90, %rd49, %rd48;
	bra.uni 	$L__BB0_7;

$L__BB0_6:
	cvt.u32.u64 	%r27, %rd48;
	cvt.u32.u64 	%r28, %rd49;
	rem.u32 	%r29, %r28, %r27;
	cvt.u64.u32 	%rd90, %r29;

$L__BB0_7:
	setp.ge.u64 	%p6, %rd5, %rd49;
	setp.ne.s64 	%p7, %rd90, 0;
	or.pred  	%p8, %p6, %p7;
	@%p8 bra 	$L__BB0_40;

	setp.gt.u64 	%p9, %rd4, %rd48;
	mov.u64 	%rd91, 1;
	@%p9 bra 	$L__BB0_12;

	and.b64  	%rd58, %rd48, -4294967296;
	setp.eq.s64 	%p10, %rd58, 0;
	@%p10 bra 	$L__BB0_11;

	div.u64 	%rd91, %rd4, %rd48;
	bra.uni 	$L__BB0_12;

$L__BB0_27:
	cvt.u32.u64 	%r43, %rd49;
	cvt.u32.u64 	%r44, %rd4;
	div.u32 	%r45, %r44, %r43;
	cvt.u64.u32 	%rd96, %r45;

$L__BB0_28:
	min.u64 	%rd32, %rd4, %rd49;
	cvt.u32.u64 	%r46, %rd32;
	cvt.u32.u64 	%r47, %rd5;
	div.u32 	%r48, %r47, %r46;
	mul.lo.s32 	%r49, %r48, %r46;
	sub.s32 	%r50, %r47, %r49;
	cvt.u64.u32 	%rd99, %r48;
	cvt.u64.u32 	%rd34, %r50;
	mul.lo.s64 	%rd35, %rd96, %rd49;
	or.b64  	%rd77, %rd48, %rd35;
	and.b64  	%rd78, %rd77, -4294967296;
	setp.eq.s64 	%p19, %rd78, 0;
	@%p19 bra 	$L__BB0_30;

	div.u64 	%rd97, %rd48, %rd35;
	bra.uni 	$L__BB0_31;

$L__BB0_30:
	cvt.u32.u64 	%r51, %rd35;
	cvt.u32.u64 	%r52, %rd48;
	div.u32 	%r53, %r52, %r51;
	cvt.u64.u32 	%rd97, %r53;

$L__BB0_31:
	mul.lo.s64 	%rd79, %rd97, %rd49;
	add.s64 	%rd80, %rd99, %rd35;
	add.s64 	%rd39, %rd80, %rd79;
	and.b64  	%rd81, %rd49, -4294967296;
	setp.eq.s64 	%p20, %rd81, 0;
	@%p20 bra 	$L__BB0_33;

	div.u64 	%rd98, %rd49, %rd32;
	bra.uni 	$L__BB0_34;

$L__BB0_33:
	cvt.u32.u64 	%r55, %rd49;
	div.u32 	%r56, %r55, %r46;
	cvt.u64.u32 	%rd98, %r56;

$L__BB0_34:
	add.s64 	%rd82, %rd34, %rd32;
	add.s64 	%rd43, %rd82, %rd98;
	setp.eq.s64 	%p21, %rd99, %rd39;
	@%p21 bra 	$L__BB0_40;

	setp.eq.s64 	%p22, %rd34, %rd43;

$L__BB0_36:
	@%p22 bra 	$L__BB0_39;

	mov.u64 	%rd100, %rd34;

$L__BB0_38:
	add.s64 	%rd83, %rd100, %rd99;
	shl.b64 	%rd84, %rd83, 3;
	add.s64 	%rd85, %rd3, %rd84;
	shl.b64 	%rd86, %rd100, 3;
	add.s64 	%rd87, %rd2, %rd86;
	ld.global.nc.f64 	%fd4, [%rd87];
	ld.global.nc.f64 	%fd5, [%rd85];
	mul.f64 	%fd6, %fd5, %fd4;
	add.s64 	%rd88, %rd1, %rd84;
	st.global.f64 	[%rd88], %fd6;
	add.s64 	%rd100, %rd100, %rd32;
	setp.ne.s64 	%p23, %rd100, %rd43;
	@%p23 bra 	$L__BB0_38;

$L__BB0_39:
	add.s64 	%rd99, %rd99, %rd35;
	setp.ne.s64 	%p24, %rd99, %rd39;
	@%p24 bra 	$L__BB0_36;
	bra.uni 	$L__BB0_40;

$L__BB0_11:
	cvt.u32.u64 	%r30, %rd48;
	cvt.u32.u64 	%r31, %rd4;
	div.u32 	%r32, %r31, %r30;
	cvt.u64.u32 	%rd91, %r32;

$L__BB0_12:
	min.u64 	%rd15, %rd4, %rd48;
	mul.lo.s64 	%rd16, %rd91, %rd48;
	or.b64  	%rd59, %rd49, %rd16;
	and.b64  	%rd60, %rd59, -4294967296;
	setp.eq.s64 	%p11, %rd60, 0;
	@%p11 bra 	$L__BB0_14;

	div.u64 	%rd92, %rd49, %rd16;
	bra.uni 	$L__BB0_15;

$L__BB0_14:
	cvt.u32.u64 	%r33, %rd16;
	cvt.u32.u64 	%r34, %rd49;
	div.u32 	%r35, %r34, %r33;
	cvt.u64.u32 	%rd92, %r35;

$L__BB0_15:
	cvt.u32.u64 	%r36, %rd15;
	cvt.u32.u64 	%r37, %rd5;
	mul.lo.s64 	%rd61, %rd92, %rd48;
	div.u32 	%r38, %r37, %r36;
	cvt.u64.u32 	%rd62, %r38;
	add.s64 	%rd63, %rd62, %rd16;
	add.s64 	%rd20, %rd63, %rd61;
	mul.lo.s32 	%r39, %r38, %r36;
	sub.s32 	%r1, %r37, %r39;
	and.b64  	%rd64, %rd48, -4294967296;
	setp.eq.s64 	%p12, %rd64, 0;
	@%p12 bra 	$L__BB0_17;

	div.u64 	%rd93, %rd48, %rd15;
	bra.uni 	$L__BB0_18;

$L__BB0_17:
	cvt.u32.u64 	%r41, %rd48;
	div.u32 	%r42, %r41, %r36;
	cvt.u64.u32 	%rd93, %r42;

$L__BB0_18:
	cvt.u64.u32 	%rd65, %r1;
	add.s64 	%rd66, %rd65, %rd15;
	add.s64 	%rd24, %rd66, %rd93;
	setp.eq.s64 	%p13, %rd20, 0;
	@%p13 bra 	$L__BB0_40;

	mov.u64 	%rd94, 0;
	setp.eq.s64 	%p14, %rd24, 0;

$L__BB0_20:
	@%p14 bra 	$L__BB0_23;

	mov.u64 	%rd95, 0;

$L__BB0_22:
	add.s64 	%rd69, %rd95, %rd94;
	shl.b64 	%rd70, %rd69, 3;
	add.s64 	%rd71, %rd2, %rd70;
	ld.global.nc.f64 	%fd1, [%rd71];
	shl.b64 	%rd72, %rd95, 3;
	add.s64 	%rd73, %rd3, %rd72;
	ld.global.nc.f64 	%fd2, [%rd73];
	mul.f64 	%fd3, %fd2, %fd1;
	add.s64 	%rd74, %rd1, %rd70;
	st.global.f64 	[%rd74], %fd3;
	add.s64 	%rd95, %rd95, %rd15;
	setp.ne.s64 	%p15, %rd95, %rd24;
	@%p15 bra 	$L__BB0_22;

$L__BB0_23:
	add.s64 	%rd94, %rd94, %rd16;
	setp.eq.s64 	%p16, %rd94, %rd20;
	@%p16 bra 	$L__BB0_40;
	bra.uni 	$L__BB0_20;

$L__BB0_40:
	ret;

}

